train <- read.csv(file = "D:\Data\Mercedes Benz\train.csv") test <- read.csv(file = "D:\Data\Mercedes Benz\test.csv")
train <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") test <- read.csv(file = "D:/Data/Mercedes Benz/test.csv")
train <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train$y id_train <- train$ID train$y <- null train$ID <- null
rm(test, envir = as.environment(".GlobalEnv"))
train$y = NULL train$ID = NULL
train_nrows <- nrow(train) train_perc <- 0.75 train <- train[, nrow * train_perc] test <- train[, nrow * (1-train_perc)]
threshold <- round(0.7 * nrow(train)) train <- train[1:threshold,] test <- train[1-(1:threshold),]
rm(train_nrows, envir = as.environment(".GlobalEnv"))
test <- train[-(1:threshold),]
train <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train$y id_train <- train$ID threshold <- round(0.7 * nrow(train)) train <- train[1:threshold,] test <- train[-(1:threshold),]
rm(test, envir = as.environment(".GlobalEnv"))
test <- train[-(1:threshold),]
test <- train[threshold:nrow(train),]
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID threshold <- round(0.7 * nrow(train_source)) train_ <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),]
rm(train_, envir = as.environment(".GlobalEnv"))
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),]
#quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL features <- names(train) #convertir caracteres como ints for (f in features) {     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))     } }
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL features <- names(train)
#convertir caracteres como ints for (f in features) {     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))     } }
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL features <- names(train)
submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv", stringsAsFactors = FALSE)
submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv")
#convertir caracteres como ints for (f in features) {     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #convertir factores como ints for (f in features) {     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
        train[[f]] = as.integer(factor(train[[f]], levels = levels))
install.packages('xgboost')
install.packages('xgboost', lib = "C:\Program Files\Microsoft\R Server\R_SERVER\library")
install.packages('xgboost', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('data.table', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('maggritr', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('magrittr', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('stringi', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(MicrosoftML) library(MicrosoftR) library(xgboost)
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } form <- "y ~" for(f in features) {     form <- form + "+"  }
form <- "y ~" for(f in features) {     form <- paste(form, f, "+") }
#quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL train$y = NULL features <- names(train)
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } form <- "y ~" for(f in features) {     form <- paste(form, f, "+") }
View(form)
form <- substr(form, 0, nrow(form) - 2)
form <- form[1:nrow(form)-2]
form <- form[1:nrow(form)-2, ]
form <- substr(form, 1, nrow(form) -2)
form <- substr(form, 1, (nrow(form) - 2))
form <- substring(form, 1, (nrow(form) - 2))
form <- `substr<-`(form, 1, (nrow(form) - 2))
substr(form, 1, (nrow(form) - 2))
form <- as.character(form)
substr(form, 1, (nchar(form) - 2))
form <- substr(form, 1, (nchar(form) - 2))
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #construir la formula form <- "y ~" for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2))
form
ft <- rxFastTrees(formula = form, data = train, type = "regression")
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) = 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (unique(train[[f]]) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (unique(train[[f]]) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     train[[f]]     nrow(unique(train[[f]]))     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     train[[f]]     nrow(unique(train[[f]]))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     print(train[[f]])     print(nrow(unique(train[[f]])))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     print(nrow(unique(train[[f]])))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     print(length(unique(train[[f]])))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]]))) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } }
form <- substr(form, 1, (nchar(form) - 2)) form
 ft <- rxFastTrees(formula = form, data = train, type = "regression")
ft
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) form ft <- rxFastTrees(formula = form, data = train, type = "regression")
scores <- rxPredict(ft, test)
#curva ROC fitROC <- rxRoc("y", "Y_Mercedes", scores) plot(fitROC)
fitROC <- rxRoc("y", grep("Probabilidad.", names(scores), value=T), scores)
fitROC <- rxRoc("y", features, scores)
fitROC <- rxRoc("y", predVarNames = "Score", scores)
fitROC <- rxRoc(actualVarName = "y", predVarNames = "Score", scores)
#curva ROC fitROC <- rxRoc(actualVarName = "Score", predVarNames = "Score", scores)
#puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test))
#curva ROC fitROC <- rxRoc(actualVarName = "y", predVarNames = "Score", scores)
fitROC <- rxRoc(actualVarName = "y", predVarNames = names(scores), scores)
actual_y <- scores$y preds <- scores$Score.rxFastTrees
install.packages('MLmetrics', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) actual_y <- scores$y preds <- scores$Score.rxFastTrees
r2 <- r2_score(y_pred = preds, y_true = actual_y)
r2 <- R2_Score(y_pred = preds, y_true = actual_y)
r2
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #convertir factores como ints     #if (is.factor(train[[f]])) {     #    levels = sort(unique(train[[f]]))     #    train[[f]] = as.integer(factor(train[[f]], levels = levels))     #    submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     #    test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) r2
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR      #convertir factores como ints     #if (is.factor(train[[f]])) {     #    levels = sort(unique(train[[f]]))     #    train[[f]] = as.integer(factor(train[[f]], levels = levels))     #    submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     #    test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees train_xgb <- xgb.DMatrix(train) test_xgb <- xgb.DMatrix(test)
train_xgb <- xgb.DMatrix(as.data.frame(train)) test_xgb <- xgb.DMatrix(as.data.frame(test))
train_xgb <- as.data.frame(train) test_xgb <- as.data.frame(test)
train_xgb <- as.data.matrix(train) test_xgb <- as.data.matrix(test)
train_xgb <- data.matrix(train) test_xgb <- data.matrix(test)
xg_model <- xgb.train(data=train_xgb, label = train$y)
train_xgb <- xgb.DMatrix(train_xgb)
xg_model <- xgb.train(data=train_xgb, label = train$y)
xg_model <- xgb.train(data=train_xgb, label = train$y, nrounds = 100, missing = NA)
train_xgb <- data.frame(train) test_xgb <- data.frame(test) train_xgb <- xgb.DMatrix(train_xgb)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb["y"]), missing = NaN)
 xgb_model <- xgb.train(data = train_xgb, nrounds = 1000,                  nfold = 6,                  early.stop.round = 10,                  objective = "reg:linear",                  print_every_n = 10,                  #num_class = 38,                  verbose = 1,                 #feval = xg_eval_mae,                 #eval = mae,                 maximize = FALSE)
xgb_model <- xgb.train(data = train_xgb, nrounds = 1000,                  nfold = 6,                  early_stop_round = 10,                  objective = "reg:linear",                  print_every_n = 10,                  #num_class = 38,                  verbose = 1,                 #feval = xg_eval_mae,                 #eval = mae,                 maximize = FALSE)
xgb_scores <- rxPredict(xgb_model, data = test, suffix = ".XGBoost",                        extraVarsToWrite = names(test))
xgb_scores <- predict(xgb_model, data = test, suffix = ".XGBoost",                        extraVarsToWrite = names(test))
xgb_scores <- predict(xgb_model, data = test)
xgb_scores <- predict(xgb_model, test)
xgb_scores <- predict(xgb_model, test_xgb)
test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(test_xgb["y"]), missing = NaN)
xgb_scores <- predict(xgb_model, test_xgb)
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR      #convertir factores como ints     #if (is.factor(train[[f]])) {     #    levels = sort(unique(train[[f]]))     #    train[[f]] = as.integer(factor(train[[f]], levels = levels))     #    submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     #    test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees train_xgb <- data.frame(train) test_xgb <- data.frame(test) train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb["y"]), missing = NaN) test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(test_xgb["y"]), missing = NaN) xgb_model <- xgb.train(data = train_xgb, nrounds = 1000,                  nfold = 6,                  early_stop_round = 10,                  objective = "reg:linear",                  print_every_n = 10,                  #num_class = 38,                  verbose = 1,                 #feval = xg_eval_mae,                 #eval = mae,                 maximize = FALSE) xgb_scores <- predict(xgb_model, test_xgb) #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  r2
#puntuar R2 r2 <- R2_Score(y_pred = xgb_scores, y_true = test_xgb$y) #mostrar la puntuacion  r2
#puntuar R2 r2_xgb <- R2_Score(y_pred = xgb_scores, y_true = test_xgb["y"]) #mostrar la puntuacion  r2_xgb
#puntuar R2 r2_xgb <- R2_Score(y_pred = xgb_scores, y_true = test$y) #mostrar la puntuacion  r2_xgb
xgb_actual_y <- test_xgb$y train_xgb$y = NULL test_xgb$y = NULL
train_xgb$y <- NULL test_xgb$y <- NULL
xgb_test_y <- test_xgb$y xgb_train_y <- train_xgb$y train_xgb <- train_xgb[, -y]
train_xgb <- train_xgb[, -c(y)]
train_xgb <- data.frame(train) test_xgb <- data.frame(test) xgb_test_y <- test_xgb$y xgb_train_y <- train_xgb$y
train_xgb <- train_xgb[, -c(y)]
label = "y"
train_xgb <- train_xgb[, -c(label)]
train_xgb <- subset(train_xgb, select = -c(y))
train_xgb <- subset(train_xgb, select = -c(y)) test_xgb <- subset(test_xgb, select = -c(y))
actual_y <- scores$y preds <- scores$Score.rxFastTrees train_xgb <- data.frame(train) test_xgb <- data.frame(test) xgb_test_y <- test_xgb$y xgb_train_y <- train_xgb$y train_xgb <- subset(train_xgb, select = -c(y)) test_xgb <- subset(test_xgb, select = -c(y))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN) test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(xgb_test_y), missing = NaN)
xgb_test_y <- as.numeric(test_xgb$y) xgb_train_y <- as.numeric(train_xgb$y)
train_xgb_df <- data.frame(train) test_xgb_df <- data.frame(test) xgb_test_y <- as.numeric(train_xgb_df$y) xgb_train_y <- as.numeric(train_xgb_df$y) train_xgb <- subset(train_xgb_df, select = -c(y)) test_xgb <- subset(train_xgb_df, select = -c(y))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb_df["y"]), missing = NaN) test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(test_xgb_df["y"]), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y))) test_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb_df["y"]), missing = NaN)
xgb_test_y <- lapply(test_xgb_df$y, as.numeric) xgb_train_y <- lapply(train_xgb_df$y, as.numeric)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb_df), label = t(xgb_train_y), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y))) test_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- lapply(data.frame(subset(train_xgb_df, select = -c(y))), as.numeric)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
summary(train_xgb)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb_df), label = t(xgb_train_y), missing = NaN)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y))) test_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- as.numeric(data.frame(subset(train_xgb_df, select = -c(y))))
train_xgb <- train_xgb_df[, -y]
train_xgb <- train_xgb_df[, train_xgb_df$y]
train_xgb <- train_xgb_df[, -train_xgb_df$y]
train_xgb_df <- data.frame(train) test_xgb_df <- data.frame(test) xgb_test_y <- lapply(test_xgb_df$y, as.numeric) xgb_train_y <- lapply(train_xgb_df$y, as.numeric) train_xgb <- train_xgb_df[, -train_xgb_df$y]
train_xgb_df <- data.frame(train) test_xgb_df <- data.frame(test) xgb_test_y <- lapply(test_xgb_df$y, as.numeric) xgb_train_y <- lapply(train_xgb_df$y, as.numeric)
train_xgb <- train_xgb_df[, -train_xgb_df$y]
train_xgb <- train_xgb_df[, 2:ncol(train_xgb_df)]
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- as.numeric(train_xgb_df[, 2:ncol(train_xgb_df)])
train_xgb <- train_xgb_df[, 2:ncol(train_xgb_df)]
train_xgb_df <- as.numeric(as.data.frame(train))
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  print(concat('RxFastTrees R2 Score', r2))
cat('RxFastTrees R2 Score', r2)
#using rxDForest() to build ML model DForest_model <- rxDForest(formula = form,                            type = "regression",                            data = train,                            seed = 1024,                            cp = 0.001,                            nTree = 200,                            mTry = 2,                            overwrite = TRUE,                            reportProgress = 0) #DForest_model #class(DForest_model) #"rxDForest"  scores <- rxPredict(DForest_model, test, suffix = ".rxDForest",                       extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxDForest #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDForest R2 Score: ', r2)
#using rxDForest() to build ML model DForest_model <- rxDForest(formula = form,                            data = train,                            seed = 1024,                            cp = 0.001,                            nTree = 200,                            mTry = 2,                            overwrite = TRUE,                            reportProgress = 0)
scores <- rxPredict(DForest_model, test, suffix = ".rxDForest",                       extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxDForest #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDForest R2 Score: ', r2)
scores <- rxPredict(DForest_model, test, #suffix = ".rxDForest",                       extraVarsToWrite = names(test))
actual_y <- scores$y preds <- scores$y_Pred #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDForest R2 Score: ', r2)
BoostedTree_model = rxBTrees(formula = form,                              data = train,                              maxDepth = 6,                              learningRate = 0.15,                              minSplit = 9,                              minBucket = 5,                              sampRate = 0.9,                              nTree = 100,                              reportProgress = 0) #BoostedTree_model #class(BoostedTree_model)
scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test))
BoostedTree_model = rxBTrees(formula = form,                              data = train,                              maxDepth = 6,                              learningRate = 0.15,                              minSplit = 9,                              minBucket = 5,                              sampRate = 0.9,                              nTree = 100,                              lossFunction = "gaussian",                              reportProgress = 0) #BoostedTree_model #class(BoostedTree_model)
dTree_model) scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test))
scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test))
actual_y <- scores$y preds <- scores$y_Pred #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxBTree R2 Score: ', r2)
DTree_model = rxDTree(formula = form,                       data = train,                       maxDepth = 6,                       minSplit = 3,                       minBucket = 3,                       nTree = 200,                       computeContext = "RxLocalParallel",                       reportProgress = 0)
scores <- rxPredict(DTree_model, test, #suffix = ".rxDTree",                       extraVarsToWrite = names(test))
actual_y <- scores$y preds <- scores$y_Pred #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDTree R2 Score: ', r2)
getwd()
wd <- getwd() source(concat(wd, "/functions.r"))
wd
concat(wd, "/functions.r")
paste(wd, "/functions.r")
 paste(wd, "/functions.r", sep = "")
source(paste(wd, "/functions.r", sep = ""))
scoreResults(scores)
source(paste(wd, "/functions.r", sep = ""))
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- paste("y~", paste(features, collapse = "+")))
form <- as.formula(paste("y~", paste(features, collapse = "+")))
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                       extraVarsToWrite = names(test))
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test))
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       #extraVarsToWrite = names(test)                       )
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )
scoreResults(scores$y, scores$Score)
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) scoreResults(scores$y, scores$Score)
scoreResults(scores$y, scores$Score)
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) scoreResults(scores$y, scores$Score)
computeR2('FastTrees', scores$y, scores$Score)
 computeR2 <- function(alg_name, actual, preds) {     #puntuar R2     r2 <- R2_Score(y_pred = preds, y_true = actual_y)     #mostrar la puntuacion      res <- cat(alg_name, ' R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', scores$y, scores$Score)
rm(scoreResults, envir = as.environment(".GlobalEnv"))
 computeR2 <- function(alg_name, actual, preds) {     #puntuar R2     r2 <- R2_Score(y_pred = preds, y_true = actual)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', scores$y, scores$Score)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     cat('actual is ' + actual_vector)     cat('vector is ' + preds_vector)     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     cat('actual is ', actual_vector)     cat('vector is ', preds_vector)     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
scores <- rxPredict(DForest_model, test, #suffix = ".rxDForest",                       extraVarsToWrite = names(test)) #sacar vectores de predicción 
computeR2('DForest', actual_vector = scores$y, preds_vector = scores$y_Pred)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(r2) }
computeR2('DForest', actual_vector = scores$y, preds_vector = scores$y_Pred)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      #res <- cat(alg_name, 'R2 Score: ', r2)     return(r2) }
computeR2('DForest', actual_vector = scores$y, preds_vector = scores$y_Pred)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) }
computeR2('RxBoostedTrees', actual_vector = scores$y, preds_vector = scores$y_Pred)
scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test)) computeR2('RxBoostedTrees', actual_vector = scores$y, preds_vector = scores$y_Pred)
scores <- rxPredict(DTree_model, test, #suffix = ".rxDTree",                       extraVarsToWrite = names(test)) computeR2('rxDTree', actual_vector = scores$y, preds_vector = scores$y_Pred)
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library/")
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library/")
install.packages("tune", lib = 'C:/Program Files/Microsoft/R Server/R_SERVER/library/')
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages("e1071", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(e1071)
library(e1071) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } fit_model <- function(alg, form, model, gr, data) {     model <- NULL     #hypertune!     model <- tune(alg, form, data = data, ranges = gr)     return(model) }
fit_model <- function(alg, form, gr, data) {     model <- NULL     #hypertune!     model <- tune(alg, form, data = data, ranges = gr)     return(model) }
fit_model("rxFastTrees", form, null, data = train)
grid <- null
grid <- NULL
fit_model("rxFastTrees", form, grid, data = train)
library(e1071)
fit_model(rxFastTrees, form, grid, data = train)
fit_model <- function(alg, form, gr, data_to_fit) {     model <- NULL     #hypertune!     model <- tune(alg, form, data = data_to_fit)     return(model) }
fit_model(rxFastTrees, form, data = train)
fit_model(rxFastTrees, form, data = train)
form2 <- paste("y~", paste(features, collapse = "+"))
#refrescar las features supervivientes features <- names(train)
#refrescar las features supervivientes features <- names(train[2:nrow(train),])
#refrescar las features supervivientes features <- names(train[,2:nrow(train)])
features <- names(train[2:nrow(train),])
features[[1]] <- NULL
features$y = NULL
features <- names(train[2:nrow(train),]) features <- features[-1]
#refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1]
#construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+")))
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
data(mcars)
data(mtcars)
rm(mtcars, envir = as.environment(".GlobalEnv"))
fit_model <- function(alg, form, gr, data_to_fit) {     model <- NULL     #hypertune!     model <- tune(alg, train.x = form, data = data_to_fit)     return(model) }
fit_model(rxFastTrees, form, data = train)
#construir la formula form <- paste("y~", paste(features, collapse = "+"))
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
form <- paste("y~", paste(features, collapse = "+"), collapse = "")
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
form <- paste("y~", paste(features, collapse = "+"), sep = "")
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
fit_model(rxFastTrees, form, data = train)
install.packages("caret", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(caret)
install.packages("rBayesianOptimization", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
names(getModelInfo())
computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) }
library(rBayesianOptimization)
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit, numBins) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0, numLeaves = numLeaves, learningRate = learningRate, minSplit = minSplit, numBins = numBins)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model <- function(alg, form, gr, data_to_fit, model_info) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3]),         numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(     fasttree_func,     bounds = bounds,     init_points = 0,     n_iter = 5,     kappa = 1     )     model <- tune(alg, train.x = form, data = data_to_fit)     return(model) }
fit_model(form, data = train)
traceback()
fasttree_func <- function(numLeaves, learningRate, minSplit, numBins = 1) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0, numLeaves = numLeaves,         learningRate = learningRate, minSplit = minSplit, numBins = numBins)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) }
fit_model(form, data = train)
traceback()
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0, numLeaves = numLeaves,         learningRate = learningRate, minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     #model <- tune(alg, train.x = form, data = data_to_fit)     return(bayes_search) }
fit_model(form, data = train)
fit_model_ft(form, data = train)
traceback()
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves = 20, learningRate = 0.1, minSplit = 5) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
fit_model_ft(form, data = train)
fasttree_func <- function(numLeaves = 20, learningRate = 0.1, minSplit = 5) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRates = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) }
fit_model_ft(form, data = train)
fit_model_ft(form, data = train)
fasttree_func <- function(numLeaves = 20, learningRate = 0.1, minSplit = 5) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) }
fit_model_ft(form, data = train)
library(caret)
#custom caret customRF <- list(type = "Regression", library = "RxFastTrees", loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"), class = c(rep("integer", 2), "numeric", rep("integer", 2)),      label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {     rxFastTrees(x, y, type = "regression", numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate, minSplit = param$minSplit, numBins = param$numBins) } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) x$classes
rm(form2, envir = as.environment(".GlobalEnv"))
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) tunegrid <- expand.grid(.numTrees = c(10:200), .numLeaves = c(5:100), .learningRate = c(0.01, 0.4), .minSplit = c(2:20), .numBins = c(16:2048)) set.seed(1234)
#custom caret customRF <- list(type = "Regression", library = "RxFastTrees", loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"), class = c(rep("integer", 2), "numeric", rep("integer", 2)),      label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {     rxFastTrees(x, y, type = "regression", numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate, minSplit = param$minSplit, numBins = param$numBins) } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) tunegrid <- expand.grid(.numTrees = c(50:150), .numLeaves = c(10:40), .learningRate = c(0.01, 0.2), .minSplit = c(5:15), .numBins = c(64:1024)) set.seed(1234)
customCaret <- train(form, data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
customRF <- list(type = "Regression", library = "RxFastTrees", loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"), class = c(rep("integer", 2), "numeric", rep("integer", 2)),      label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {     rxFastTrees(formula = form, data = train, type = "regression", numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate, minSplit = param$minSplit, numBins = param$numBins) } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) tunegrid <- expand.grid(.numTrees = c(50:150), .numLeaves = c(10:40), .learningRate = c(0.01, 0.2), .minSplit = c(5:15), .numBins = c(64:1024)) set.seed(1234)
customCaret <- train(form, data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
customRF$fit <- function(form, train, wts, param, lev, last, weights, classProbs, ...) {     rxFastTrees(formula = form, data = train, type = "regression", numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate, minSplit = param$minSplit, numBins = param$numBins) }
customCaret <- train(form, data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
customRF$grid <- function(len = NULL, search = "grid") { }
customCaret <- train(form, data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
customCaret <- train(form = form, data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
customRF <- list(type = "Regression", library = "RxFastTrees", loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"), class = c(rep("integer", 2), "numeric", rep("integer", 2)),      label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(len = NULL, search = "grid") { } customRF$fit <- function(form, train, wts, param, lev, last, weights, classProbs, ...) {     rxFastTrees(formula = form, data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) x$classes
set.seed(1234) customCaret <- train(form = form, data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
customCaret <- train(paste("y~", paste(features, collapse = "+"), sep = ""), data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
print(form)
customCaret <- train(y ~ ., data=train, method=customRF, metric="Rsquared", tuneGrid = tunegrid, trControl = control)
#custom caret customRF <- list(type = "Regression", library = "RxFastTrees", loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"), class = c(rep("integer", 2), "numeric", rep("integer", 2)),      label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(len = NULL, search = "grid") { } customRF$fit <- function(form, train, wts, param, lev, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(formula = form, data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) x$classes
customCaret <- train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)
#custom caret customRF <- list(type = "Regression", library = "MicrosoftML", loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"), class = c(rep("integer", 2), "numeric", rep("integer", 2)),      label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(len = NULL, search = "grid") { } customRF$fit <- function(form, train, wts, param, lev, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(formula = form, data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) x$classes
customCaret <- train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)
customCaret <- train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control, replace=FALSE)
traceback()
customCaret <- train.formula(form=form., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test,                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(paramlist, i) {     if (length(i) > 1L)         return(lapply(i, fit_model_ft))     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]] } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
#construir mejores parametros  bestParams <- list() best_r2 <- 0.0
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test,                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(paramlist, i) {     if (length(i) > 1L)         return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     i <- i + 1     return(sc, bestParams) } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
hyperparams <- apply(tunegrid, 1, fit_model_ft)
fit_model_ft <- function(paramlist) {     if (length(i) > 1L)         return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     return(sc, bestParams) }
fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     return(sc, bestParams) }
hyperparams <- apply(tunegrid, 1, fit_model_ft)
#grid de exploracion tunegrid <- expand.grid(numTrees = c(50:150), numLeaves = c(10:40), learningRate = c(0.01, 0.2), minSplit = c(5:15), numBins = c(64:1024))
hyperparams <- apply(tunegrid, 1, fit_model_ft)
computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #devuelve la puntuacion      return(r2) }
hyperparams <- apply(tunegrid, 1, fit_model_ft)
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #devuelve la puntuacion      return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test,                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     ret <- as.data.frame(sc, bestParams)     return(ret) } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
hyperparams <- apply(tunegrid, 1, fit_model_ft)
tunegrid <- expand.grid(numTrees = c(100, 110, 120, 130, 140, 150), numLeaves = c(20:35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8:12), numBins = c(256, 512, 1024))
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(form, train, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(formula = form, data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } customRF$prob <- NULL #ES UN REGRESOR, PROB NO APLICA  #function(modelFit, newdata, preProc = NULL, submodels = NULL){     #rxPredict(modelFit, newdata, type = "prob") #} customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
customCaret <- train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") }
customCaret <- train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)
warnings()
x #custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(formula = x, data = y, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) RxComputeContext("RxLocalParallel")
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(formula = x, data = y, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
RxComputeContext('localpar')
customCaret <- train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)
warnings()
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(formula = form, data = x, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) RxComputeContext('localpar')
customCaret <- rxExec(train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control))
warnings()
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(y ~ ., data = x, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) RxComputeContext('localpar')
customCaret <- rxExec(train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control))
warnings()
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(y ~ ., data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) RxComputeContext('localpar')
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #devuelve la puntuacion      return(r2) } fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- numTrees     resultsTuning$numLeaves[i] <- numLeaves     resultsTuning$learningRate[i] <- learningRate     resultsTuning$minSplit[i] <- minSplit     resultsTuning$numBins[i] <- numBins     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     } } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test,                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     ret <- as.data.frame(sc, bestParams)     return(ret) } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) library(e1071) library(caret) #ejecucion local paralela  RxComputeContext('localpar') #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1] #construir la formula form <- paste("y~", paste(features, collapse = "+"), sep = "") #construir mejores parametros  bestParams <- list() best_r2 <- 0.0 #grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 110, 120, 130, 140, 150), numLeaves = c(20:35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8:12), numBins = c(256, 512, 1024))
resultsTuning <- data.frame() numExecutions <- nrows(tunegrid)
numExecutions <- nrow(tunegrid)
#version paralela con RxExec rxExec(fit_model_parallel, RxElemArgs(nrows(tunegrid)))
rrxExec(fit_model_parallel, rxElemArg(nrows(tunegrid)))
rxExec(fit_model_parallel, rxElemArg(nrows(tunegrid)))
rxExec(fit_model_parallel, rxElemArg(1:numExecutions))
tunegrid$numTrees[1]
    print(paste('trying now with', tunegrid$numTrees[1], sep=""))
    print(paste('trying now with', tunegrid$numTrees[1], sep=" "))
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- numTrees     resultsTuning$numLeaves[i] <- numLeaves     resultsTuning$learningRate[i] <- learningRate     resultsTuning$minSplit[i] <- minSplit     resultsTuning$numBins[i] <- numBins     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     } }
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #devuelve la puntuacion      return(r2) } fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     } } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test,                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     ret <- as.data.frame(sc, bestParams)     return(ret) } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
rxExec(fit_model_parallel, rxElemArg(1:numExecutions))
rxExec(fit_model_parallel, rxElemArg(numExecutions))
resultsTuning$numTrees <- NULL resultsTuning$numLeaves <- NULL resultsTuning$learningRate <- NULL resultsTuning$minSplit <- NULL resultsTuning$numBins <- NULL
resultsTuning <- as.data.frame(NULL)
resultsTuning$numTrees <- 0 resultsTuning$numLeaves <- 0 resultsTuning$learningRate <- 0 resultsTuning$minSplit <- 0 resultsTuning$numBins <- 0
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) library(e1071) library(caret) #ejecucion local paralela  RxComputeContext('localpar') #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1] #construir la formula form <- paste("y~", paste(features, collapse = "+"), sep = "") #construir mejores parametros  bestParams <- list() best_r2 <- 0.0 #grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 110, 120, 130, 140, 150), numLeaves = c(20:35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8:12), numBins = c(256, 512, 1024))
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #devuelve la puntuacion      return(r2) } fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     } } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test,                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     ret <- as.data.frame(sc, bestParams)     return(ret) } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) library(e1071) library(caret) #ejecucion local paralela  RxComputeContext('localpar') #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1] #construir la formula form <- paste("y~", paste(features, collapse = "+"), sep = "") #construir mejores parametros  bestParams <- list() best_r2 <- 0.0 #grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 110, 120, 130, 140, 150), numLeaves = c(20:35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8:12), numBins = c(256, 512, 1024))
resultsTuning <- data.frame(numTrees=integer(), numLeaves = integer(), learningRate = double(), minSplit = integer(), numBins = integer())
numExecutions <- nrow(tunegrid)
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      #print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     } }
rxExec(fit_model_parallel, rxElemArg(numExecutions))
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) library(e1071) library(caret) #ejecucion local paralela  RxComputeContext('localpar') #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1] #construir la formula form <- paste("y~", paste(features, collapse = "+"), sep = "") #construir mejores parametros  bestParams <- list() best_r2 <- 0.0 #grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 110, 120, 130, 140, 150), numLeaves = c(20:35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8:12), numBins = c(256, 512, 1024))
resultsTuning <- data.frame(numTrees=integer(), numLeaves = integer(), learningRate = double(), minSplit = integer(), numBins = integer()) numExecutions <- nrow(tunegrid)
resultsTuning <- data.frame(numTrees = integer(numExecutions), numLeaves = integer(numExecutions),     learningRate = double(numExecutions), minSplit = integer(numExecutions), numBins = integer(numExecutions))
rxExec(fit_model_parallel, rxElemArg(numExecutions))
resultsTuning <- data.frame(numTrees = integer(numExecutions), numLeaves = integer(numExecutions),     learningRate = double(numExecutions), minSplit = integer(numExecutions), numBins = integer(numExecutions)     , r2 = double(numExecutions))
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      #print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     resultsTuning$r2[i] <- sc }
rxExec(fit_model_parallel, rxElemArg(numExecutions))
tunegrid$numBins[1] <- 1
tunegrid$numBins[1] = 1
tunegrid$numBins[1]
tunegrid$numBins[[1]] = 1
tunegrid$numBins[["1"]] = 1
tunegrid$numBins[[1]] = 1
rxExec(fit_model_parallel, i = rxElemArg(1:numExecutions))
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      #print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     resultsTuning$r2[i] <- sc     if (sc > best_r2) {          best_r2 <- sc         bestParams <- list(tunegrid$numTrees[i], tunegrid$numLeaves[i], tunegrid$learningRate[i], tunegrid$minSplit[i], tunegrid$numBins[i])     } }
rxExec(fit_model_parallel, i = rxElemArg(1:numExecutions))
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) library(e1071) library(caret) #ejecucion local paralela  RxComputeContext('localpar') #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1] #construir la formula form <- paste("y~", paste(features, collapse = "+"), sep = "") #construir mejores parametros  bestParams <- list() best_r2 <- 0.0 #grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 110, 120, 130, 140, 150), numLeaves = c(20:35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8:12), numBins = c(256, 512, 1024))
numExecutions <- nrow(tunegrid) resultsTuning <- data.frame(numTrees = integer(numExecutions), numLeaves = integer(numExecutions),     learningRate = double(numExecutions), minSplit = integer(numExecutions), numBins = integer(numExecutions)     , r2 = double(numExecutions))
rxExec(fit_model_parallel, i = rxElemArg(1:numExecutions))
time(rxExec(fit_model_parallel, i = rxElemArg(1:numExecutions)))
tunegrid$r2 <- double(nrow(tunegrid))
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      #print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     #resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     #resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     #resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     #resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     #resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     tunegrid$r2[i] <- sc     if (sc > best_r2) {          best_r2 <- sc         bestParams <- list(tunegrid$numTrees[i], tunegrid$numLeaves[i], tunegrid$learningRate[i], tunegrid$minSplit[i], tunegrid$numBins[i])     } }
tunegrid$r2[1]
tunegrid$r2[1] <- 3
tunegrid$r2[1]
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "data.RData"))     #train_p <- subset(tempd_data, tag =      #print(paste('trying now with', tunegrid$numTrees[], sep=" "))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     #resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     #resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     #resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     #resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     #resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     tunegrid$r2[i] <- sc     if (sc > best_r2) {          best_r2 <- sc         bestParams <- list(tunegrid$numTrees[i], tunegrid$numLeaves[i], tunegrid$learningRate[i], tunegrid$minSplit[i], tunegrid$numBins[i])     } }
resultsTuning$numTrees[1]
temp_data <- readRDS(paste(wd, "data.RData"))
temp_data <- readRDS(paste(wd, "/data.RData", sep=""))
fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "/data.RData", sep=""))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     #resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     #resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     #resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     #resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     #resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     tunegrid$r2[i] <- sc     if (sc > best_r2) {          best_r2 <- sc         bestParams <- list(tunegrid$numTrees[i], tunegrid$numLeaves[i], tunegrid$learningRate[i], tunegrid$minSplit[i], tunegrid$numBins[i])     } }
tunegrid <- expand.grid(numTrees = c(100, 120, 140, 160), numLeaves = c(20, 30, 35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8, 10, 12), numBins = c(256, 512, 1024)) #añadir puntuaciones vacias tunegrid$r2 <- double(nrow(tunegrid))
#numero de sweeps por el grid numExecutions <- nrow(tunegrid)
#version paralela con RxExec time(rxExec(fit_model_parallel, i = rxElemArg(1:numExecutions)))
tunegrid$r2[300]
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(y ~ ., data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins, verbose = 0)     resultsTuning$numTrees[i] <- param$numTrees     resultsTuning$numLeaves[i] <- param$numLeaves     resultsTuning$learningRate[i] <- param$learningRate     resultsTuning$minSplit[i] <- param$minSplit     resultsTuning$numBins[i] <- param$numBins } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) customCaret <- rxExec(train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control))
#grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 120, 140, 160), numLeaves = c(20, 30, 35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8, 10, 12), numBins = c(256, 512, 1024))
#grid de exploracion tunegrid <- expand.grid(numTrees = c(100, 120, 140, 160), numLeaves = c(20, 30, 35), learningRate = c(0.1, 0.15, 0.2), minSplit = c(8, 10, 12), numBins = c(256, 512, 1024)) #añadir puntuaciones vacias #tunegrid$r2 <- double(nrow(tunegrid)) #numero de sweeps por el grid numExecutions <- nrow(tunegrid) #para cada elemento del grid, aplicar fit del modelo y evaluar su rendimiento hyperparams <- apply(tunegrid, 1, fit_model_ft) #custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(y ~ ., data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins, verbose = 0)     resultsTuning$numTrees[i] <- param$numTrees     resultsTuning$numLeaves[i] <- param$numLeaves     resultsTuning$learningRate[i] <- param$learningRate     resultsTuning$minSplit[i] <- param$minSplit     resultsTuning$numBins[i] <- param$numBins } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) customCaret <- rxExec(train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control))
warnings()
time(hyperparams <- apply(tunegrid, 1, fit_model_ft))
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #devuelve la puntuacion      return(r2) } fit_model_parallel <- function(i) {     #temp_data <- readRDS(paste(wd, "/data.RData", sep=""))     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = tunegrid$numTrees[i], numLeaves = tunegrid$numLeaves[i], learningRate = tunegrid$learningRate[i],         minSplit = tunegrid$minSplit[i], numBins = tunegrid$numBins[i], verbose = 0)     #resultsTuning$numTrees[i] <- tunegrid$numTrees[i]     #resultsTuning$numLeaves[i] <- tunegrid$numLeaves[i]     #resultsTuning$learningRate[i] <- tunegrid$learningRate[i]     #resultsTuning$minSplit[i] <- tunegrid$minSplit[i]     #resultsTuning$numBins[i] <- tunegrid$numBins[i]     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     tunegrid$r2[i] <- sc     if (sc > best_r2) {          best_r2 <- sc         bestParams <- list(tunegrid$numTrees[i], tunegrid$numLeaves[i], tunegrid$learningRate[i], tunegrid$minSplit[i], tunegrid$numBins[i])     } } fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- paramlist     }     ret <- as.data.frame(sc, bestParams)     return(ret) } fit_model_ft_Bayesian <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
#custom caret customRF <- list(type = "Regression", library = c("MicrosoftML", "MicrosoftR"), loop = NULL) customRF$parameters <- data.frame(parameter = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins"),             class = c(rep("integer", 2), "numeric", rep("integer", 2)),             label = c("numTrees", "numLeaves", "learningRate", "minSplit", "numBins")) customRF$grid <- function(x, y, len = NULL, search = "grid") { } customRF$fit <- function(x, y, wts, param, lev = NULL, last, weights, classProbs, ...) {     MicrosoftML::rxFastTrees(y ~ ., data = train, type = "regression",         numTrees = param$numTrees, numLeaves = param$numLeaves, learningRate = param$learningRate,         minSplit = param$minSplit, numBins = param$numBins, verbose = 0) } customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {     rxPredict(modelFit, newdata) } #ES UN REGRESOR, PROB NO APLICA  customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL){     rxPredict(modelFit, newdata, type = "prob") } customRF$sort <- function(x) x[order(x[, 1]),] customRF$levels <- function(x) lev(x) # x$classes control <- trainControl(method = "repeatedcv", number = 10, repeats = 3) time(customCaret <- rxExec(train(y ~ ., data = train, method = customRF, metric = "Rsquared", tuneGrid = tunegrid, trControl = control)))
warnings()
time(hyperparams <- apply(tunegrid, 1, fit_model_ft))
fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- list(current_numTrees, current_numLeaves, current_learningRate, current_minSplit, current_numBins)     } }
fit_model_ft <- function(paramlist) {     #if (length(i) > 1L)     #    return(lapply(i, fit_model_ft))     current_numTrees <- paramlist[["numTrees"]]     current_numLeaves <- paramlist[["numLeaves"]]     current_learningRate <- paramlist[["learningRate"]]     current_minSplit <- paramlist[["minSplit"]]     current_numBins <- paramlist[["numBins"]]     m <- rxFastTrees(formula = form, data = train, type = "regression",         numTrees = current_numTrees, numLeaves = current_numLeaves, learningRate = current_learningRate,         minSplit = current_minSplit, numBins = current_numBins)     scores <- rxPredict(m, test, extraVarsToWrite = names(test))     sc <- computeR2_silent(actual_vector = scores$y, preds_vector = scores$Score)     if (sc > best_r2) {         best_r2 <- sc         bestParams <- list(current_numTrees, current_numLeaves, current_learningRate, current_minSplit, current_numBins)     } }
time(hyperparams <- apply(tunegrid, 1, fit_model_ft))
best_r2
?readRDS
temp_data <- readRDS(paste(wd, "/data.RData", sep=""))
