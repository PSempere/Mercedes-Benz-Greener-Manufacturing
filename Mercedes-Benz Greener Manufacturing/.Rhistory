train <- read.csv(file = "D:\Data\Mercedes Benz\train.csv") test <- read.csv(file = "D:\Data\Mercedes Benz\test.csv")
train <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") test <- read.csv(file = "D:/Data/Mercedes Benz/test.csv")
train <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train$y id_train <- train$ID train$y <- null train$ID <- null
rm(test, envir = as.environment(".GlobalEnv"))
train$y = NULL train$ID = NULL
train_nrows <- nrow(train) train_perc <- 0.75 train <- train[, nrow * train_perc] test <- train[, nrow * (1-train_perc)]
threshold <- round(0.7 * nrow(train)) train <- train[1:threshold,] test <- train[1-(1:threshold),]
rm(train_nrows, envir = as.environment(".GlobalEnv"))
test <- train[-(1:threshold),]
train <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train$y id_train <- train$ID threshold <- round(0.7 * nrow(train)) train <- train[1:threshold,] test <- train[-(1:threshold),]
rm(test, envir = as.environment(".GlobalEnv"))
test <- train[-(1:threshold),]
test <- train[threshold:nrow(train),]
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID threshold <- round(0.7 * nrow(train_source)) train_ <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),]
rm(train_, envir = as.environment(".GlobalEnv"))
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),]
#quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL features <- names(train) #convertir caracteres como ints for (f in features) {     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))     } }
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL features <- names(train)
#convertir caracteres como ints for (f in features) {     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))     } }
train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL features <- names(train)
submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv", stringsAsFactors = FALSE)
submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv")
#convertir caracteres como ints for (f in features) {     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #convertir factores como ints for (f in features) {     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
        train[[f]] = as.integer(factor(train[[f]], levels = levels))
install.packages('xgboost')
install.packages('xgboost', lib = "C:\Program Files\Microsoft\R Server\R_SERVER\library")
install.packages('xgboost', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('data.table', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('maggritr', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('magrittr', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages('stringi', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(MicrosoftML) library(MicrosoftR) library(xgboost)
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } form <- "y ~" for(f in features) {     form <- form + "+"  }
form <- "y ~" for(f in features) {     form <- paste(form, f, "+") }
#quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL train$y = NULL features <- names(train)
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } form <- "y ~" for(f in features) {     form <- paste(form, f, "+") }
View(form)
form <- substr(form, 0, nrow(form) - 2)
form <- form[1:nrow(form)-2]
form <- form[1:nrow(form)-2, ]
form <- substr(form, 1, nrow(form) -2)
form <- substr(form, 1, (nrow(form) - 2))
form <- substring(form, 1, (nrow(form) - 2))
form <- `substr<-`(form, 1, (nrow(form) - 2))
substr(form, 1, (nrow(form) - 2))
form <- as.character(form)
substr(form, 1, (nchar(form) - 2))
form <- substr(form, 1, (nchar(form) - 2))
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #construir la formula form <- "y ~" for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2))
form
ft <- rxFastTrees(formula = form, data = train, type = "regression")
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) = 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (unique(train[[f]]) = 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (unique(train[[f]]) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     train[[f]]     nrow(unique(train[[f]]))     #quitar columnas con un solo valor     if (nrow(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     train[[f]]     nrow(unique(train[[f]]))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
#preprocesado for (f in features) {     print(train[[f]])     print(nrow(unique(train[[f]])))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     print(nrow(unique(train[[f]])))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     print(length(unique(train[[f]])))     #quitar columnas con un solo valor     #if (nrow(unique(train[[f]])) == 1) {     #    train[[f]] = NULL     #    submission_core[[f]] = NULL     #}     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]]))) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } }
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } }
form <- substr(form, 1, (nchar(form) - 2)) form
 ft <- rxFastTrees(formula = form, data = train, type = "regression")
ft
library(MicrosoftML) library(MicrosoftR) library(xgboost) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) form ft <- rxFastTrees(formula = form, data = train, type = "regression")
scores <- rxPredict(ft, test)
#curva ROC fitROC <- rxRoc("y", "Y_Mercedes", scores) plot(fitROC)
fitROC <- rxRoc("y", grep("Probabilidad.", names(scores), value=T), scores)
fitROC <- rxRoc("y", features, scores)
fitROC <- rxRoc("y", predVarNames = "Score", scores)
fitROC <- rxRoc(actualVarName = "y", predVarNames = "Score", scores)
#curva ROC fitROC <- rxRoc(actualVarName = "Score", predVarNames = "Score", scores)
#puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test))
#curva ROC fitROC <- rxRoc(actualVarName = "y", predVarNames = "Score", scores)
fitROC <- rxRoc(actualVarName = "y", predVarNames = names(scores), scores)
actual_y <- scores$y preds <- scores$Score.rxFastTrees
install.packages('MLmetrics', lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #convertir factores como ints     if (is.factor(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     }     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) actual_y <- scores$y preds <- scores$Score.rxFastTrees
r2 <- r2_score(y_pred = preds, y_true = actual_y)
r2 <- R2_Score(y_pred = preds, y_true = actual_y)
r2
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #print(train[[f]])     #print(length(unique(train[[f]])))     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #convertir factores como ints     #if (is.factor(train[[f]])) {     #    levels = sort(unique(train[[f]]))     #    train[[f]] = as.integer(factor(train[[f]], levels = levels))     #    submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     #    test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) r2
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR      #convertir factores como ints     #if (is.factor(train[[f]])) {     #    levels = sort(unique(train[[f]]))     #    train[[f]] = as.integer(factor(train[[f]], levels = levels))     #    submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     #    test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees train_xgb <- xgb.DMatrix(train) test_xgb <- xgb.DMatrix(test)
train_xgb <- xgb.DMatrix(as.data.frame(train)) test_xgb <- xgb.DMatrix(as.data.frame(test))
train_xgb <- as.data.frame(train) test_xgb <- as.data.frame(test)
train_xgb <- as.data.matrix(train) test_xgb <- as.data.matrix(test)
train_xgb <- data.matrix(train) test_xgb <- data.matrix(test)
xg_model <- xgb.train(data=train_xgb, label = train$y)
train_xgb <- xgb.DMatrix(train_xgb)
xg_model <- xgb.train(data=train_xgb, label = train$y)
xg_model <- xgb.train(data=train_xgb, label = train$y, nrounds = 100, missing = NA)
train_xgb <- data.frame(train) test_xgb <- data.frame(test) train_xgb <- xgb.DMatrix(train_xgb)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb["y"]), missing = NaN)
 xgb_model <- xgb.train(data = train_xgb, nrounds = 1000,                  nfold = 6,                  early.stop.round = 10,                  objective = "reg:linear",                  print_every_n = 10,                  #num_class = 38,                  verbose = 1,                 #feval = xg_eval_mae,                 #eval = mae,                 maximize = FALSE)
xgb_model <- xgb.train(data = train_xgb, nrounds = 1000,                  nfold = 6,                  early_stop_round = 10,                  objective = "reg:linear",                  print_every_n = 10,                  #num_class = 38,                  verbose = 1,                 #feval = xg_eval_mae,                 #eval = mae,                 maximize = FALSE)
xgb_scores <- rxPredict(xgb_model, data = test, suffix = ".XGBoost",                        extraVarsToWrite = names(test))
xgb_scores <- predict(xgb_model, data = test, suffix = ".XGBoost",                        extraVarsToWrite = names(test))
xgb_scores <- predict(xgb_model, data = test)
xgb_scores <- predict(xgb_model, test)
xgb_scores <- predict(xgb_model, test_xgb)
test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(test_xgb["y"]), missing = NaN)
xgb_scores <- predict(xgb_model, test_xgb)
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) train <- train_source[1:threshold,] test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR      #convertir factores como ints     #if (is.factor(train[[f]])) {     #    levels = sort(unique(train[[f]]))     #    train[[f]] = as.integer(factor(train[[f]], levels = levels))     #    submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))     #    test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression") #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                        extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees train_xgb <- data.frame(train) test_xgb <- data.frame(test) train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb["y"]), missing = NaN) test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(test_xgb["y"]), missing = NaN) xgb_model <- xgb.train(data = train_xgb, nrounds = 1000,                  nfold = 6,                  early_stop_round = 10,                  objective = "reg:linear",                  print_every_n = 10,                  #num_class = 38,                  verbose = 1,                 #feval = xg_eval_mae,                 #eval = mae,                 maximize = FALSE) xgb_scores <- predict(xgb_model, test_xgb) #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  r2
#puntuar R2 r2 <- R2_Score(y_pred = xgb_scores, y_true = test_xgb$y) #mostrar la puntuacion  r2
#puntuar R2 r2_xgb <- R2_Score(y_pred = xgb_scores, y_true = test_xgb["y"]) #mostrar la puntuacion  r2_xgb
#puntuar R2 r2_xgb <- R2_Score(y_pred = xgb_scores, y_true = test$y) #mostrar la puntuacion  r2_xgb
xgb_actual_y <- test_xgb$y train_xgb$y = NULL test_xgb$y = NULL
train_xgb$y <- NULL test_xgb$y <- NULL
xgb_test_y <- test_xgb$y xgb_train_y <- train_xgb$y train_xgb <- train_xgb[, -y]
train_xgb <- train_xgb[, -c(y)]
train_xgb <- data.frame(train) test_xgb <- data.frame(test) xgb_test_y <- test_xgb$y xgb_train_y <- train_xgb$y
train_xgb <- train_xgb[, -c(y)]
label = "y"
train_xgb <- train_xgb[, -c(label)]
train_xgb <- subset(train_xgb, select = -c(y))
train_xgb <- subset(train_xgb, select = -c(y)) test_xgb <- subset(test_xgb, select = -c(y))
actual_y <- scores$y preds <- scores$Score.rxFastTrees train_xgb <- data.frame(train) test_xgb <- data.frame(test) xgb_test_y <- test_xgb$y xgb_train_y <- train_xgb$y train_xgb <- subset(train_xgb, select = -c(y)) test_xgb <- subset(test_xgb, select = -c(y))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN) test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(xgb_test_y), missing = NaN)
xgb_test_y <- as.numeric(test_xgb$y) xgb_train_y <- as.numeric(train_xgb$y)
train_xgb_df <- data.frame(train) test_xgb_df <- data.frame(test) xgb_test_y <- as.numeric(train_xgb_df$y) xgb_train_y <- as.numeric(train_xgb_df$y) train_xgb <- subset(train_xgb_df, select = -c(y)) test_xgb <- subset(train_xgb_df, select = -c(y))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb_df["y"]), missing = NaN) test_xgb <- xgb.DMatrix(data.matrix(test_xgb), label = t(test_xgb_df["y"]), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y))) test_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(train_xgb_df["y"]), missing = NaN)
xgb_test_y <- lapply(test_xgb_df$y, as.numeric) xgb_train_y <- lapply(train_xgb_df$y, as.numeric)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb_df), label = t(xgb_train_y), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y))) test_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- lapply(data.frame(subset(train_xgb_df, select = -c(y))), as.numeric)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
summary(train_xgb)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb_df), label = t(xgb_train_y), missing = NaN)
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- data.frame(subset(train_xgb_df, select = -c(y))) test_xgb <- data.frame(subset(train_xgb_df, select = -c(y)))
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- as.numeric(data.frame(subset(train_xgb_df, select = -c(y))))
train_xgb <- train_xgb_df[, -y]
train_xgb <- train_xgb_df[, train_xgb_df$y]
train_xgb <- train_xgb_df[, -train_xgb_df$y]
train_xgb_df <- data.frame(train) test_xgb_df <- data.frame(test) xgb_test_y <- lapply(test_xgb_df$y, as.numeric) xgb_train_y <- lapply(train_xgb_df$y, as.numeric) train_xgb <- train_xgb_df[, -train_xgb_df$y]
train_xgb_df <- data.frame(train) test_xgb_df <- data.frame(test) xgb_test_y <- lapply(test_xgb_df$y, as.numeric) xgb_train_y <- lapply(train_xgb_df$y, as.numeric)
train_xgb <- train_xgb_df[, -train_xgb_df$y]
train_xgb <- train_xgb_df[, 2:ncol(train_xgb_df)]
train_xgb <- xgb.DMatrix(data.matrix(train_xgb), label = t(xgb_train_y), missing = NaN)
train_xgb <- as.numeric(train_xgb_df[, 2:ncol(train_xgb_df)])
train_xgb <- train_xgb_df[, 2:ncol(train_xgb_df)]
train_xgb_df <- as.numeric(as.data.frame(train))
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- substr(form, 1, (nchar(form) - 2)) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxFastTrees #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  print(concat('RxFastTrees R2 Score', r2))
cat('RxFastTrees R2 Score', r2)
#using rxDForest() to build ML model DForest_model <- rxDForest(formula = form,                            type = "regression",                            data = train,                            seed = 1024,                            cp = 0.001,                            nTree = 200,                            mTry = 2,                            overwrite = TRUE,                            reportProgress = 0) #DForest_model #class(DForest_model) #"rxDForest"  scores <- rxPredict(DForest_model, test, suffix = ".rxDForest",                       extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxDForest #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDForest R2 Score: ', r2)
#using rxDForest() to build ML model DForest_model <- rxDForest(formula = form,                            data = train,                            seed = 1024,                            cp = 0.001,                            nTree = 200,                            mTry = 2,                            overwrite = TRUE,                            reportProgress = 0)
scores <- rxPredict(DForest_model, test, suffix = ".rxDForest",                       extraVarsToWrite = names(test)) #sacar vectores de predicción  actual_y <- scores$y preds <- scores$Score.rxDForest #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDForest R2 Score: ', r2)
scores <- rxPredict(DForest_model, test, #suffix = ".rxDForest",                       extraVarsToWrite = names(test))
actual_y <- scores$y preds <- scores$y_Pred #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDForest R2 Score: ', r2)
BoostedTree_model = rxBTrees(formula = form,                              data = train,                              maxDepth = 6,                              learningRate = 0.15,                              minSplit = 9,                              minBucket = 5,                              sampRate = 0.9,                              nTree = 100,                              reportProgress = 0) #BoostedTree_model #class(BoostedTree_model)
scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test))
BoostedTree_model = rxBTrees(formula = form,                              data = train,                              maxDepth = 6,                              learningRate = 0.15,                              minSplit = 9,                              minBucket = 5,                              sampRate = 0.9,                              nTree = 100,                              lossFunction = "gaussian",                              reportProgress = 0) #BoostedTree_model #class(BoostedTree_model)
dTree_model) scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test))
scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test))
actual_y <- scores$y preds <- scores$y_Pred #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxBTree R2 Score: ', r2)
DTree_model = rxDTree(formula = form,                       data = train,                       maxDepth = 6,                       minSplit = 3,                       minBucket = 3,                       nTree = 200,                       computeContext = "RxLocalParallel",                       reportProgress = 0)
scores <- rxPredict(DTree_model, test, #suffix = ".rxDTree",                       extraVarsToWrite = names(test))
actual_y <- scores$y preds <- scores$y_Pred #puntuar R2 r2 <- R2_Score(y_pred = preds, y_true = actual_y) #mostrar la puntuacion  cat('rxDTree R2 Score: ', r2)
getwd()
wd <- getwd() source(concat(wd, "/functions.r"))
wd
concat(wd, "/functions.r")
paste(wd, "/functions.r")
 paste(wd, "/functions.r", sep = "")
source(paste(wd, "/functions.r", sep = ""))
scoreResults(scores)
source(paste(wd, "/functions.r", sep = ""))
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #construir la formula form <- "y ~" #refrescar las features supervivientes features <- names(train) for (f in features) {     if (f != "y") {         form <- paste(form, f, "+")     } } form <- paste("y~", paste(features, collapse = "+")))
form <- as.formula(paste("y~", paste(features, collapse = "+")))
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, suffix = ".rxFastTrees",                       extraVarsToWrite = names(test))
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test))
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       #extraVarsToWrite = names(test)                       )
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )
scoreResults(scores$y, scores$Score)
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) scoreResults(scores$y, scores$Score)
scoreResults(scores$y, scores$Score)
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) scoreResults(scores$y, scores$Score)
computeR2('FastTrees', scores$y, scores$Score)
 computeR2 <- function(alg_name, actual, preds) {     #puntuar R2     r2 <- R2_Score(y_pred = preds, y_true = actual_y)     #mostrar la puntuacion      res <- cat(alg_name, ' R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', scores$y, scores$Score)
rm(scoreResults, envir = as.environment(".GlobalEnv"))
 computeR2 <- function(alg_name, actual, preds) {     #puntuar R2     r2 <- R2_Score(y_pred = preds, y_true = actual)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', scores$y, scores$Score)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     cat('actual is ' + actual_vector)     cat('vector is ' + preds_vector)     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     cat('actual is ', actual_vector)     cat('vector is ', preds_vector)     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(res) }
library(MicrosoftML) library(MicrosoftR) library(xgboost) library(MLmetrics) #cargamos fichero functions wd <- getwd() source(paste(wd, "/functions.r", sep = "")) #leemos dato train_source <- read.csv(file = "D:/Data/Mercedes Benz/train.csv") submission_core <- read.csv(file = "D:/Data/Mercedes Benz/test.csv") label = "y" label_train <- train_source$y id_train <- train_source$ID #70% para entrenar threshold <- round(0.7 * nrow(train_source)) set.seed(1234) #division aleatoria train_indicator <- sample(1:nrow(train_source), size = threshold) train <- train_source[train_indicator,] test <- train_source[-train_indicator,] #division secuencial #train <- train_source[1:threshold,] #test <- train_source[(threshold + 1):nrow(train_source),] #quitamos para que no afecte al entrenamiento, es autonumerica train$ID = NULL test$ID = NULL #train$y = NULL features <- names(train) #preprocesado for (f in features) {     #quitar columnas con un solo valor     if (length(unique(train[[f]])) == 1) {         train[[f]] = NULL         submission_core[[f]] = NULL         test[[f]] = NULL     }     #COMENTADO, A FASTTREES LE VA MEJOR CON FACTORES      #convertir factores como ints     #if (is.factor(train[[f]])) {         #levels = sort(unique(train[[f]]))         #train[[f]] = as.integer(factor(train[[f]], levels = levels))         #submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         #test[[f]] = as.integer(factor(test[[f]], levels = levels))     #}     #convertir characters como ints     if (is.character(train[[f]])) {         levels = sort(unique(train[[f]]))         train[[f]] = as.integer(factor(train[[f]], levels = levels))         submission_core[[f]] = as.integer(factor(submission_core[[f]], levels = levels))         test[[f]] = as.integer(factor(test[[f]], levels = levels))     } } #refrescar las features supervivientes features <- names(train) #construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+"))) #entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0) #puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
scores <- rxPredict(DForest_model, test, #suffix = ".rxDForest",                       extraVarsToWrite = names(test)) #sacar vectores de predicción 
computeR2('DForest', actual_vector = scores$y, preds_vector = scores$y_Pred)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- cat(alg_name, 'R2 Score: ', r2)     return(r2) }
computeR2('DForest', actual_vector = scores$y, preds_vector = scores$y_Pred)
 computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      #res <- cat(alg_name, 'R2 Score: ', r2)     return(r2) }
computeR2('DForest', actual_vector = scores$y, preds_vector = scores$y_Pred)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) }
computeR2('RxBoostedTrees', actual_vector = scores$y, preds_vector = scores$y_Pred)
scores <- rxPredict(BoostedTree_model, test, #suffix = ".rxBTrees",                       extraVarsToWrite = names(test)) computeR2('RxBoostedTrees', actual_vector = scores$y, preds_vector = scores$y_Pred)
scores <- rxPredict(DTree_model, test, #suffix = ".rxDTree",                       extraVarsToWrite = names(test)) computeR2('rxDTree', actual_vector = scores$y, preds_vector = scores$y_Pred)
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library/")
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library/")
install.packages("tune", lib = 'C:/Program Files/Microsoft/R Server/R_SERVER/library/')
install.packages("tune", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
install.packages("e1071", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(e1071)
library(e1071) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } fit_model <- function(alg, form, model, gr, data) {     model <- NULL     #hypertune!     model <- tune(alg, form, data = data, ranges = gr)     return(model) }
fit_model <- function(alg, form, gr, data) {     model <- NULL     #hypertune!     model <- tune(alg, form, data = data, ranges = gr)     return(model) }
fit_model("rxFastTrees", form, null, data = train)
grid <- null
grid <- NULL
fit_model("rxFastTrees", form, grid, data = train)
library(e1071)
fit_model(rxFastTrees, form, grid, data = train)
fit_model <- function(alg, form, gr, data_to_fit) {     model <- NULL     #hypertune!     model <- tune(alg, form, data = data_to_fit)     return(model) }
fit_model(rxFastTrees, form, data = train)
fit_model(rxFastTrees, form, data = train)
form2 <- paste("y~", paste(features, collapse = "+"))
#refrescar las features supervivientes features <- names(train)
#refrescar las features supervivientes features <- names(train[2:nrow(train),])
#refrescar las features supervivientes features <- names(train[,2:nrow(train)])
features <- names(train[2:nrow(train),])
features[[1]] <- NULL
features$y = NULL
features <- names(train[2:nrow(train),]) features <- features[-1]
#refrescar las features supervivientes features <- names(train) #quitamos Y features <- features[-1]
#construir la formula form <- as.formula(paste("y~", paste(features, collapse = "+")))
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
data(mcars)
data(mtcars)
rm(mtcars, envir = as.environment(".GlobalEnv"))
fit_model <- function(alg, form, gr, data_to_fit) {     model <- NULL     #hypertune!     model <- tune(alg, train.x = form, data = data_to_fit)     return(model) }
fit_model(rxFastTrees, form, data = train)
#construir la formula form <- paste("y~", paste(features, collapse = "+"))
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
form <- paste("y~", paste(features, collapse = "+"), collapse = "")
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
form <- paste("y~", paste(features, collapse = "+"), sep = "")
#entrenamiento con valores por defecto ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0)
#puntuar scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       ) computeR2('FastTrees', actual_vector = scores$y, preds_vector = scores$Score)
fit_model(rxFastTrees, form, data = train)
fit_model(rxFastTrees, form, data = train)
install.packages("caret", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
library(caret)
install.packages("rBayesianOptimization", lib = "C:/Program Files/Microsoft/R Server/R_SERVER/library")
names(getModelInfo())
computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) }
library(rBayesianOptimization)
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit, numBins) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0, numLeaves = numLeaves, learningRate = learningRate, minSplit = minSplit, numBins = numBins)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model <- function(alg, form, gr, data_to_fit, model_info) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3]),         numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(     fasttree_func,     bounds = bounds,     init_points = 0,     n_iter = 5,     kappa = 1     )     model <- tune(alg, train.x = form, data = data_to_fit)     return(model) }
fit_model(form, data = train)
traceback()
fasttree_func <- function(numLeaves, learningRate, minSplit, numBins = 1) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0, numLeaves = numLeaves,         learningRate = learningRate, minSplit = minSplit, numBins = numBins)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) }
fit_model(form, data = train)
traceback()
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves, learningRate, minSplit) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0, numLeaves = numLeaves,         learningRate = learningRate, minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     #model <- tune(alg, train.x = form, data = data_to_fit)     return(bayes_search) }
fit_model(form, data = train)
fit_model_ft(form, data = train)
traceback()
library(e1071) library(caret) library(rBayesianOptimization) computeR2 <- function(alg_name, actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(res) } computeR2_silent <- function(actual_vector, preds_vector) {     #puntuar R2     r2 <- R2_Score(y_pred = preds_vector, y_true = actual_vector)     #mostrar la puntuacion      res <- paste(alg_name, 'R2 Score:', r2)     return(r2) } fasttree_func <- function(numLeaves = 20, learningRate = 0.1, minSplit = 5) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) } fit_model_ft <- function(form, data_to_fit) {     upperBounds <- c(     numLeaves = 100,     learningRate = 0.4,     minSplit = 20,     numBins = 2048     )     lowerBounds <- c(     numLeaves = 5,     learningRate = 0.001,     minSplit = 2,     numBins = 16     )     bounds <- list(         numLeaves = c(lowerBounds[1], upperBounds[1]),         learningRate = c(lowerBounds[2], upperBounds[2]),         minSplit = c(lowerBounds[3], upperBounds[3])         #,numBins = c(lowerBounds[4], upperBounds[4])         )     set.seed(1234)     #hypertune!     bayes_search <- BayesianOptimization(                         fasttree_func,                         bounds = bounds,                         init_points = 0,                         n_iter = 5,                         kappa = 1     )     return(bayes_search) }
fit_model_ft(form, data = train)
fasttree_func <- function(numLeaves = 20, learningRate = 0.1, minSplit = 5) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRates = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) }
fit_model_ft(form, data = train)
fit_model_ft(form, data = train)
fasttree_func <- function(numLeaves = 20, learningRate = 0.1, minSplit = 5) {     ft <- rxFastTrees(formula = form, data = train, type = "regression", verbose = 0,         numLeaves = numLeaves,         learningRate = learningRate,         minSplit = minSplit)     scores <- rxPredict(ft, test, #suffix = ".rxFastTrees",                       extraVarsToWrite = names(test)                       )     list(Score = computeR2_silent(scores$y, scores$Score), Pred = scores$Score) }
fit_model_ft(form, data = train)
